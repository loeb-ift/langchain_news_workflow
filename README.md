# AI 新聞稿自動生成工作流程 (LangChain 版本)

## 專案目的

本專案旨在提供一個完整的自動化新聞稿生成流程。它利用大型語言模型（LLM），將輸入的原始資料通過 Alpha、Beta、Gamma、Delta 四個階段的專業處理，最終產出一篇結構完整、風格明確、品質優良的新聞稿。

系統支援雙模式操作：互動模式允許使用者在每個階段審核、修改、重試結果，確保最終產出符合期望；非互動模式則直接處理輸入資料並輸出完整結果，適用於批量處理和自動化場景。

## 功能特色

- **四階段處理流程**：
  - **Alpha (資訊架構師)**：將原始資料轉為結構化的新聞初稿。
  - **Beta (風格塑造師)**：將初稿依據指定風格（如：經濟日報、數位時代）進行改寫。
  - **Gamma (標題策略師)**：產生多種類型（新聞型、數據型等）的標題選項。
  - **Delta (品質守門員)**：進行最終的品質審核、修正與定稿。
- **雙模式支援**：
  - **互動模式**：使用者可以在每個階段後暫停，選擇接受、重試、或修改內容。
  - **非互動模式**：直接處理輸入資料並輸出完整結果，適用於批量處理和自動化場景。
- **可配置性**：支援透過命令列參數調整新聞類型、目標風格、語氣、字數等。
- **本地模型支援**：使用 [Ollama](https://ollama.com/) 在本地端運行語言模型，無需依賴 OpenAI 等雲端服務。
- **決策日誌**：可選擇性地將每次執行的決策過程記錄下來，存成 `pipeline_log.csv` 檔案，便於分析與追蹤。
- **Web UI 介面**：透過 Gradio 提供友好的使用者介面，簡化操作流程。
- **測試框架**：包含完整的單元測試和集成測試，確保程式穩定性。

## 核心架構

### 四階段處理流程

本專案的核心是其獨特的四階段處理流程，每個階段由不同角色的「AI 助手」負責：

#### 1. Alpha (資訊架構師)
- **目的**：深度分析原始資料，識別所有關鍵資訊點
- **任務**：建立資訊架構（核心亮點 → 支援細節 → 背景脈絡）
- **輸出**：結構化的新聞初稿、關鍵點列表、資訊層級分析
- **特色**：根據不同新聞類型（財經、科技、產業等）調整處理重點

#### 2. Beta (風格塑造師)
- **目的**：將初稿依據指定風格進行改寫
- **任務**：調整語調、用詞和敘述方式，使其符合目標媒體的風格
- **支援的風格**：經濟日報、中央社、數位時代、券商研報等
- **特色**：可根據不同語氣要求（客觀中性、積極正面、謹慎保守）調整文字風格

#### 3. Gamma (標題策略師)
- **目的**：產生吸引人且符合新聞特性的標題
- **任務**：創作四種不同類型的標題（新聞型、數據型、趨勢型、影響型）
- **特色**：自動生成SEO關鍵字，標題長度控制在15-25字

#### 4. Delta (品質守門員)
- **目的**：進行最終的品質審核、修正與定稿
- **任務**：檢查語法、修辭、一致性，並提供品質報告
- **輸出**：最終新聞稿、品質報告（字數、合規性、可讀性、專業度）
- **特色**：支援使用者提供修正方向，精準調整內容

### 提示詞管理系統

專案使用靈活的提示詞管理系統，支援：

- 基礎提示詞模板（位於 `prompts/` 目錄）
- 依據新聞類型、目標風格、語氣的動態提示詞組合
- 提示詞覆蓋機制（透過 `prompts/overrides/` 目錄自訂配置）
- 會話級別的提示詞追加（如修正方向）

## 檔案結構

專案採用模組化架構設計，將核心功能、工具類和界面分離，便於維護和擴展。以下是主要檔案和目錄說明：

```
.
├── .env                   # 環境變數設定檔（本地配置）
├── .env.example           # 環境變數範例檔（參考模板）
├── README.md              # 專案說明文件
├── app_utils/             # 共用工具模組，提供核心功能支持
│   ├── json_utils.py      # JSON 安全解析與格式化工具
│   ├── ollama_utils.py    # Ollama 模型交互及錯誤處理工具
│   ├── prompt_manager.py  # 提示詞動態組合與管理系統
│   ├── ui_texts.json      # UI 界面文字配置
│   └── ui_texts.py        # UI 文字加載與管理
├── article.txt            # 原始文章範例（測試用）
├── backend/               # 後端服務模組
│   ├── main.py            # 後端API主程式
│   └── requirements.txt   # 後端專用依賴清單
├── gradio_app.py          # Gradio Web UI 主程序
├── logs/                  # 日誌存儲目錄
│   ├── details/           # 詳細JSON格式執行日誌
│   └── run.csv            # 運行記錄彙總CSV
├── pipeline.py            # 主要新聞生成腳本（互動/非互動模式）
├── pipeline_log.py        # 增強版腳本（支援日誌記錄與批次處理）
├── pipeline_log.csv       # 決策過程記錄CSV（預設路徑）
├── prompts/               # AI提示詞模板庫（JSON格式）
│   ├── alpha.json         # Alpha階段（資訊架構師）提示詞
│   ├── beta.json          # Beta階段（風格塑造師）提示詞
│   ├── delta.json         # Delta階段（品質守門員）提示詞
│   ├── gamma.json         # Gamma階段（標題策略師）提示詞
│   └── overrides/         # 自定義提示詞覆蓋配置
├── requirements.txt       # 主專案Python套件依賴清單
├── server.py              # 後端伺服器啟動腳本
└── tests/                 # 自動化測試用例
    ├── conftest.py        # 測試全局配置
    ├── smoke_test.py      # 基礎功能煙霧測試
    ├── test_edge_cases.py # 邊界條件與異常處理測試
    ├── test_pipeline_mock.py  # 模擬LLM回應的流程測試
    └── test_pipeline_variations.py  # 多場景變化測試
```

## 環境設定

### 基本配置

1. 克隆專案到本地（或直接下載源碼）：
   ```bash
   git clone [repository_url]
   cd langchain_news_workflow
   ```

2. 安裝必要的 Python 依賴套件：
   ```bash
   pip install -r requirements.txt
   ```

3. 配置環境變數：
   - 複製範例配置文件：`cp .env.example .env`
   - 使用文字編輯器打開 `.env` 文件，根據您的實際環境調整設置
   - 關鍵配置包括：
     ```
     # 您本地 Ollama 服務的位址
     OLLAMA_BASE_URL="http://localhost:11434"
     # 您希望使用的模型名稱
     OLLAMA_MODEL_NAME="llama3:8b"
     # pipeline_log.py 中 CSV 日誌的預設路徑
     PIPELINE_LOG_CSV="pipeline_log.csv"
     # 是否開啟 LLM 除錯模式（顯示完整提示詞）
     LLM_DEBUG=0
     ```

### 模型準備

4. 確保 Ollama 服務已成功啟動，並已下載所需的語言模型：
   - 推薦使用兼容的大型語言模型，如 llama3:8b（預設）
   - 模型名稱和配置將在 `.env` 文件中指定

### 自定義設置（可選）

5. 提示词配置：
   - 默認提示词模板位於 `prompts/` 目錄，採用 JSON 格式存儲，每個模板包含以下部分：
     - `base`：基礎提示詞模板，定義角色和主要任務
     - `by_news_type`：依據新聞類型調整的提示詞
     - `by_target_style`：依據目標媒體風格調整的提示詞
     - `by_tone`：依據語氣要求調整的提示詞
   - 使用 `prompts/overrides/` 目錄可以為特定場景提供自定義提示词覆蓋

## 使用說明

本專案提供三種執行模式：

### 1. Web UI 介面 (`gradio_app.py`)

提供功能完整的圖形化界面，適合所有使用者，包含即時預覽、互動式配置和提示詞管理功能。

**啟動指令：**

```bash
python gradio_app.py
```

啟動後，將自動開啟瀏覽器顯示 Web 介面，或手動訪問 http://localhost:7860

#### Web UI 主要功能

**📝 單篇文章處理**
- 原始文章內容輸入框，支援長文本粘貼
- 配置選項：新聞類型、目標媒體風格、語氣風格、目標字數、特殊限制
- 處理結果展示：最終標題、最終內容（可直接編輯）、品質報告、處理日誌
- 進階設定：Ollama 服務位址配置、模型選擇、除錯模式

**📁 批量處理文件**
- 支援上傳多個 .txt 文件
- 統一配置處理參數
- 結果以 CSV 格式呈現，便於後續處理

**🛠️ 提示词管理**
- 階段管理：查看、編輯、創建和管理提示词階段
- 基础提示词：System 和 User 提示词编辑
- 高級配置：按新聞類型、目標風格、語氣的追加提示词配置
- 自定義配置：在不修改原始文件的情況下保存個性化配置

#### Web UI 特色功能

- **Ollama 模型管理**：支援刷新和選擇可用模型，實時顯示連接狀態
- **會話數據匯出**：將當前處理過程完整數據以 JSON 格式匯出
- **互動式編輯**：處理結果可直接在界面上編輯和調整
- **實時反饋**：處理過程中的日誌和錯誤訊息即時顯示

#### 界面進階配置
- 默認服務器位址：127.0.0.1
- 默認端口：7860
- 啟動後自動打開瀏覽器

### 2. 命令列模式 (`pipeline.py`)

直接在命令列中執行新聞稿生成流程，適用於批次處理或與其他系統整合。命令列模式支持互動模式和非互動模式，可透過參數切換。

#### 互動模式

互動模式允許您在每個處理階段查看、修改和確認結果，適用於需要人工審核的場景：

```bash
python pipeline.py \
  --raw-data "您的原始資料內容" \
  --news-type "財經" \
  --target-style "經濟日報" \
  --word-limit 150
```

#### 非互動模式

非互動模式自動完成所有處理階段，直接輸出完整新聞稿，適用於自動化和批量處理：

```bash
python pipeline.py \
  --raw-data "您的原始資料內容" \
  --news-type "財經" \
  --target-style "經濟日報" \
  --word-limit 150 \
  --no-interactive
```

#### 命令列參數說明

| 參數名稱 | 格式 | 說明 | 預設值 | 必需性 |
|----------|------|------|--------|--------|
| `--raw-data` | 字串 | 原始資料內容，可以是文字、新聞報導或會議記錄 | - | 是 |
| `--news-type` | 字串 | 新聞類型（如：財經、科技、政治等） | "一般" | 否 |
| `--target-style` | 字串 | 目標媒體風格（如：經濟日報、科技新聞等） | "綜合報導" | 否 |
| `--word-limit` | 數字 | 目標字數限制（建議 100-1000 字） | 500 | 否 |
| `--tone` | 字串 | 語氣設定（如：專業、輕鬆、客觀等） | "客觀" | 否 |
| `--interactive / --no-interactive` | 開關 | 是否開啟互動模式 | True | 否 |
| `--model` | 字串 | 要使用的 Ollama 模型名稱 | 環境變數 OLLAMA_MODEL_NAME | 否 |
| `--api-base` | URL | Ollama API 基礎 URL | 環境變數 OLLAMA_BASE_URL | 否 |
| `--temperature` | 數字 | 生成回應的隨機性（0-1，值越低越確定） | 0.7 | 否 |

非互動模式的輸出為 JSON 格式，包含完整的處理結果，方便後續程式處理。

### 3. 增强版命令列模式 (`pipeline_log.py`)

`pipeline_log.py` 是在 `pipeline.py` 的基础上，增加了決策過程記錄和批次處理功能的增強版腳本，特別適用於需要記錄處理過程、批量處理多個文件或整合到自動化工作流中的場景。

#### 主要特性

**核心功能增強：**
- 完整記錄每個處理階段的決策和結果到 CSV 日志文件
- 支援批量處理多個輸入文件或整個資料夾
- 自動保存處理過程中的所有關鍵數據
- 提供更詳細的運行統計和錯誤處理機制

#### 基本使用示例

**單次文本處理（互動模式）**
```bash
python pipeline_log.py \
  --raw-data "$(cat article.txt)" \
  --news-type "財經" \
  --target-style "經濟日報" \
  --word-limit 700 \
  --tone "客觀中性"
```

**批次處理整個資料夾（非互動模式）**
```bash
python pipeline_log.py \
  --files ./folder_with_txts \
  --non-interactive
```

**批次處理並保存詳細記錄**
```bash
python pipeline_log.py \
  --files ./folder_with_txts \
  --non-interactive \
  --log-csv ./logs/my_pipeline_log.csv \
  --json-out-dir ./logs/details
```

**使用環境變數配置**
```bash
export PIPELINE_LOG_CSV=./logs/batch_log.csv
export OLLAMA_MOCK=true
python pipeline_log.py --files ./folder_with_txts --non-interactive
```

#### 批次處理特性說明

使用 `--files` 參數可以輕鬆處理多個文件：
- 支持指定單個文件、多個文件或整個資料夾路徑
- 處理資料夾時會自動遞迴查找所有 `.txt` 文件
- 在非互動模式下可以無人值守地處理大量文件
- 所有處理過程都會詳細記錄到 CSV 文件便於後續分析
- 使用 `--json-out-dir` 可保存每個處理任務的完整詳細記錄

#### 日志文件說明

**CSV 日志文件包含以下關鍵信息：**
- 處理時間和持續時間
- 初始輸入資料摘要
- 各階段處理決策記錄
- 最終標題和內容摘要
- 處理狀態和可能的錯誤信息

#### 命令列參數說明

以下是 `pipeline.py` 和 `pipeline_log.py` 共用的主要參數：

| 參數 | 說明 | 預設值 | 
| :--- | :--- | :--- | 
| `--raw-data` | 原始資料內容 | （必填） | 
| `--news-type` | 新聞類型，會影響 AI 處理的側重點 | `財經` | 
| `--target-style` | 目標媒體風格，AI 會模仿其寫作風格 | `經濟日報` | 
| `--word-limit` | 希望的目標字數 | `800` | 
| `--tone` | 希望的語氣要求 | `客觀中性` | 
| `--constraints` | 其他特殊限制或要求 | `None` | 
| `--additional-answers-json` | JSON 字串形式的補充資訊 | `None` | 
| `--max-retries` | 各階段自動重試次數 | `2` | 
| `--interactive/--no-interactive` | 是否啟用互動模式（僅 pipeline.py） | `True` | 
| `--non-interactive` | 非互動模式（僅 pipeline_log.py） | `False` | 
| `--ollamaHost` | 指定 Ollama 服務位址（含 port），覆蓋環境變數 `OLLAMA_BASE_URL` |  | 
| `--model` | 指定模型名稱，覆蓋環境變數 `OLLAMA_MODEL_NAME` |  | 
| `--show-prompts` | 顯示 LLM 提示詞預覽（除錯用），預設不顯示 | `False` | 

`pipeline_log.py` 特有的參數：

| 參數 | 說明 | 預設值 | 
| :--- | :--- | :--- | 
| `--files` | 要處理的檔案或資料夾路徑（可多個）；資料夾會遞迴讀取所有 `.txt` 檔 |  | 
| `--log-csv` | CSV 輸出路徑；若未提供，讀取環境變數 `PIPELINE_LOG_CSV`；再未提供則為 `pipeline_log.csv` |  | 
| `--json-out-dir` | 詳細 JSON 日誌的輸出目錄 |  | 

#### 常見用法
- 單次文字 + 指定 CSV 與 JSON 詳錄（指定遠端 Ollama 主機）
```bash
python pipeline_log.py \
  --raw-data "$(cat article.txt)" \
  --log-csv ./logs/run.csv \
  --json-out-dir ./logs/details \
  --ollamaHost http://10.0.0.5:11434
```

- 批次處理資料夾所有 .txt（非互動）+ 指定 CSV 與 JSON 詳錄
```bash
python pipeline_log.py \
  --files ./folder_with_txts \
  --non-interactive \
  --log-csv ./logs/batch.csv \
  --json-out-dir ./logs/details
```

- 多個路徑混合（檔案與資料夾）
```bash
python pipeline_log.py \
  --files ./a.txt \
  --files ./folder_b \
  --non-interactive \
  --log-csv ./logs/mixed.csv
```

- 透過環境變數指定預設 CSV 路徑
```bash
export PIPELINE_LOG_CSV=./logs/default.csv
python pipeline_log.py --files ./folder --non-interactive
```

- 模擬模式（不呼叫模型）
```bash
export OLLAMA_MOCK=true
python pipeline_log.py --files ./folder --non-interactive
```

#### CSV 欄位說明（成功與失敗皆會寫入）
- `session_id`: 本次處理的唯一識別
- `start_time` / `end_time` / `duration_seconds`: 執行區間與耗時（秒）
- `initial_raw_data`: 初始輸入（為了可讀性已截斷至 250 字）
- `alpha_decisions` / `beta_decisions` / `gamma_decisions` / `delta_decisions`: 各階段的決策摘要（JSON）。若使用 `--json-out-dir`，每一筆 `log_entries` 會包含各階段完整 prompt 與模型回覆，存成一個 session_id.json。
- `final_headline`: 最終標題（成功時）
- `final_body`: 最終內文（成功時）或 `[FAILED] stage=... message=...`（失敗/中止時）

## 互動流程範例

以下是在互動模式下的典型流程：

### 1. 初始化

首先，你需要提供原始資料、新聞類型、目標風格等參數。

```
提供原始資料： 
"""
台積電今天宣布，將在2023年投資700億美元，在全球多地擴大產能。其中，美國亞利桑那州廠區將成為重要據點，計畫生產最先進的3nm晶片。
"""

新聞類型: 財經
目標風格: 經濟日報
字數限制: 300
語氣要求: 客觀中性
```

### 2. Alpha 階段

Alpha 階段完成後，系統會顯示處理後的結果，並提供操作選項。

```
===== Alpha 階段 (資訊架構師) 結果 =====

核心亮點:
- 台積電計畫2023年投資700億美元在全球擴大產能
- 美國亞利桑那州廠區將成為重要據點
- 將在當地生產最先進的3nm晶片

支援細節:
- 投資金額具體為700億美元
- 產能擴張計畫涵蓋全球多地
- 3nm晶片代表業界最先進的製程技術

背景脈絡:
- 這是台積電全球布局策略的重要一步
- 美國廠區建設反映全球晶片供應鏈重組趨勢

新聞初稿:
台積電今日宣布，將於2023年投入700億美元資金，在全球多個地點擴大晶片產能。其中，位於美國亞利桑那州的廠區將成為這一全球戰略的重要據點，並計畫在當地生產最先進的3nm晶片。

這一投資計畫標誌著台積電全球布局戰略的重要一步，尤其是美國廠區的建設，也反映了全球晶片供應鏈正在經歷的重組趨勢。700億美元的投資規模顯示出台積電在半導體行業的領導地位和長遠規劃。

Alpha 階段操作: [接受(a)/修改(m)/重試(r)/退出(q)]: a
```

你可以選擇接受(a)、修改(m)或重試(r)結果。

### 3. Beta 階段

Beta 階段完成後，系統同樣會顯示處理後的結果，並提供操作選項。

```
===== Beta 階段 (風格塑造師) 結果 =====

[經濟日報風格] 台積電宣布2023年鉅額投資700億美元 全球產能擴張 亞利桑那廠將量產3nm晶片

台積電今日對外宣布，2023年將啟動規模高達700億美元的全球產能擴張計畫，涵蓋多個海外據點建設。其中，位於美國亞利桑那州的新廠區將成為此次擴張的關鍵據點，計畫在當地量產最先進的3nm製程晶片。

據了解，這一鉅額投資不僅體現出台積電作為全球半導體龍頭企業的實力與雄心，更是其全球布局戰略的重要一步。業內分析指出，亞利桑那廠的建設與3nm產能的導入，反映了全球晶片供應鏈正處於深度調整期，各大廠商均在積極調整生產與投資布局。

值得注意的是，700億美元的年度投資規模，創下台積電歷年投資新高，顯示出公司對半導體產業長期發展的信心。

Beta 階段操作：[接受(a)/修改(m)/重試(r)/退出(q)]: a
```

### 4. Gamma 階段

Gamma 階段主要生成標題，同樣提供操作選項。

```
===== Gamma 階段 (標題策略師) 結果 =====

1. [新聞型] 台積電2023年投資700億美元 全球產能擴張 亞利桑那廠將量產3nm晶片
2. [數據型] 700億美元! 台積電啟動史上最大投資計畫 全球多地同步擴產
3. [趨勢型] 台積電加速全球化布局 700億美元投資推動高端晶片產能擴張
4. [影響型] 台積電700億美元全球擴產 將如何改變半導體產業格局

SEO 關鍵字: 台積電, 投資, 700億美元, 全球產能, 亞利桑那, 3nm晶片

Gamma 階段操作：[選擇1-4/全部接受(a)/修改(m)/重試(r)/退出(q)]: 1
```

你可以選擇一個最喜歡的標題，或者接受全部標題。

### 5. Delta 階段

Delta 階段是最後的品質審核階段。

```
===== Delta 階段 (品質守門員) 結果 =====

最終新聞稿:
台積電宣布2023年鉅額投資700億美元 全球產能擴張 亞利桑那廠將量產3nm晶片

台積電今日對外宣布，2023年將啟動規模高達700億美元的全球產能擴張計畫，涵蓋多個海外據點建設。其中，位於美國亞利桑那州的新廠區將成為此次擴張的關鍵據點，計畫在當地量產最先進的3nm製程晶片。

據了解，這一鉅額投資不僅體現出台積電作為全球半導體龍頭企業的實力與雄心，更是其全球布局戰略的重要一步。業內分析指出，亞利桑那廠的建設與3nm產能的導入，反映了全球晶片供應鏈正處於深度調整期，各大廠商均在積極調整生產與投資布局。

值得注意的是，700億美元的年度投資規模，創下台積電歷年投資新高，顯示出公司對半導體產業長期發展的信心。

品質報告:
- 字數: 256
- 語法正確性: 優良
- 風格一致性: 符合經濟日報風格
- 內容完整性: 完整覆蓋所有關鍵資訊
- 建議: 可適當增加一些行業分析觀點

Delta 階段操作：[接受(a)/修改(m)/重試(r)/退出(q)]: a
```

### 6. 完成

所有階段完成後，系統會輸出最終結果。

```
所有階段處理完成！

最終新聞稿已複製到剪貼簿。

===== 最終結果摘要 =====

標題: 台積電宣布2023年鉅額投資700億美元 全球產能擴張 亞利桑那廠將量產3nm晶片
新聞類型: 財經
目標風格: 經濟日報
字數: 256

感謝使用本系統！
```

## 非交互式模式使用範例

非交互式模式適用於批量處理和自動化場景，直接輸出完整的JSON格式結果。

### 基本用法

```bash
python pipeline.py \
  --raw-data "$(cat article.txt)" \
  --news-type "財經" \
  --target-style "經濟日報" \
  --word-limit 140 \
  --no-interactive
```

### 輸出格式

非交互式模式的輸出是JSON格式，包含以下主要字段：

```json
{
  "success": true,
  "data": {
    "final_body": "最終新聞稿內容...",
    "best_title": "最終選定的標題",
    "headline_options": ["標題選項1", "標題選項2"...],
    "seo_keywords": ["關鍵字1", "關鍵字2"...],
    "quality_report": {
      "word_count": 256,
      "grammar_score": "優良",
      "style_consistency": "符合經濟日報風格",
      "content_completeness": "完整",
      "suggestions": "建議內容..."
    },
    "publishable": true
  }
}
```

## 常用工具模組

本專案包含幾個核心工具模組，位於 `app_utils/` 目錄：

### 1. json_utils.py

提供安全的JSON解析和格式化工具，處理可能的JSON解析錯誤和格式不完整的情況。

### 2. ollama_utils.py

與Ollama模型交互的工具，包含模型調用、回應解析和錯誤處理等功能。

### 3. prompt_manager.py

負責管理和組合提示詞模板，支援依據新聞類型、目標風格和語氣動態調整提示詞。

### 4. ui_texts.py 和 ui_texts.json

管理UI界面和交互過程中的文本，支援多語言和自定義文本。

## 測試

本專案包含完整的測試框架，可確保功能正常運作：

### 測試的特性

- **模擬模式**：測試會使用模擬的 LLM 回應，確保不需要真實的 Ollama 服務也能運行
- **綜合覆蓋**：包含煙霧測試、邊界條件測試和變化場景測試
- **非交互式模式**：測試使用非交互式模式，確保整個流程可以自動運行

### 如何運行測試

在專案根目錄下執行以下命令：

```bash
# 執行所有測試
pytest

# 更詳細的輸出
python -m pytest -v

# 排除特定測試
python -m pytest -v -k 'not <test_name>'

# 執行特定測試
pytest tests/smoke_test.py
```

### 測試用例說明

- `smoke_test.py`：基本功能測試，確保整個流程可以正常運行
- `test_edge_cases.py`：邊界條件測試，例如處理空資料、極端字數要求等
- `test_pipeline_mock.py`：使用模擬回應的測試，用於快速驗證流程
- `test_pipeline_variations.py`：各種變化場景的測試，例如不同的新聞類型、風格等

### 測試結果解釋

成功的測試應顯示所有測試用例通過，類似於：

```
collected 10 items                                                                                                                           

tests/smoke_test.py::test_smoke PASSED                                                                                                  
...
```

如果有測試失敗，請檢查相應的錯誤訊息，可能需要調整測試或代碼。

## 後端服務

如需單獨啟動後端服務：

```bash
python server.py
```

## 常見問題

### 1. 為什麼我的測試失敗了？

可能的原因包括：

- 模擬回應不匹配：測試使用模擬的 LLM 回應，如果提示詞或回應格式改變，可能導致測試失敗。
- 環境變數設置：確保環境變數設置正確，尤其是 `OLLAMA_BASE_URL` 和 `OLLAMA_MODEL_NAME`。
- 依賴套件版本：確保使用正確版本的依賴套件，可以透過 `pip install -r requirements.txt` 重新安裝。

如果遇到測試失敗，可以嘗試重新運行測試，或者查看詳細的錯誤訊息以找出問題所在。

### 2. 如何自訂提示詞？

你可以在 `prompts/overrides/` 目錄中添加相應的 JSON 檔案，以覆蓋默認的提示詞。覆蓋的提示詞將與默認提示詞合併，未指定的部分將使用默認值。

### 3. 如何支援更多的新聞類型或風格？

可以透過以下步驟添加新的新聞類型或風格：

1. 在 `prompts/overrides/` 目錄中創建新的 JSON 檔案
2. 在新的 JSON 檔案中添加相應的 `by_news_type` 或 `by_target_style` 配置
3. 確保新的配置與現有的提示詞格式兼容

### 4. 如何在自動化工作流中使用本專案？

你可以使用非交互式模式在自動化工作流中集成本專案：

```bash
python pipeline.py \
  --raw-data "原始資料內容" \
  --news-type "財經" \
  --target-style "經濟日報" \
  --no-interactive > output.json
```

然後使用其他腳本解析 `output.json` 檔案，提取所需的資訊。

### 5. 如何處理大量文件？

對於大量文件的處理，建議使用 `pipeline_log.py` 腳本的批次處理功能：

```bash
python pipeline_log.py \
  --files data/news_articles/ \
  --news-type "財經" \
  --target-style "經濟日報" \
  --non-interactive \
  --log-csv batch_process_log.csv \
  --json-out-dir batch_results/
```

## 使用建議

### 選擇適合的模式

- **Web UI**：適合初次使用、需要直覺式操作的用戶
- **互動式命令列**：適合需要精細控制、希望在每個階段進行調整的用戶
- **非互動式命令列**：適合自動化處理、批量生成新聞稿的場景
- **批處理模式**：適合處理大量文件，需要詳細記錄的場景

### 提高生成品質的技巧

1. **提供清晰的原始資料**：越詳細、結構越清晰的原始資料，生成的新聞稿品質越好
2. **選擇適當的新聞類型和風格**：根據內容選擇合適的新聞類型，根據目標受眾選擇合適的風格
3. **善用互動功能**：在關鍵階段使用重試和修改功能，引導生成更符合期望的內容
4. **自訂提示詞**：根據特殊需求，透過提示詞覆蓋功能調整AI的行為

### 性能優化建議

1. **使用合適的Ollama模型**：根據需求選擇適當大小的模型，平衡品質和速度
2. **調整批處理參數**：處理大量文件時，可適當增加`--max-retries`參數的值
3. **設置合理的字數限制**：字數限制過高可能影響生成速度和品質

## 總結

本專案提供了一個完整的AI新聞稿自動生成工作流程，透過四階段處理和多種使用模式，滿足不同用戶的需求。它的主要優勢在於：

- **靈活多樣的使用方式**：Web UI、互動式命令列、非互動式命令列和批處理模式
- **高度可配置**：支援調整新聞類型、目標風格、語氣、字數等多個參數
- **本地模型支援**：使用Ollama在本地運行模型，保護數據隱私
- **完善的測試框架**：確保功能穩定可靠
- **開放的擴展機制**：支援自訂提示詞和添加新的新聞類型/風格

無論是需要精細控制的個別新聞稿生成，還是需要高效處理的批量任務，本專案都能提供合適的解決方案。
